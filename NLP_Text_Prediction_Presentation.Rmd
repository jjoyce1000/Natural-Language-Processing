---
title: "NLP Text Prediction"
author: "John Joyce"
date: "March 26, 2018"
output: 
    ioslides_presentation:
    smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Introduction
* Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be cumbersome. SwiftKey, the corporate partner of the Johns Hopkins Data Science capstone, builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models.  

* This data science capstone project analyzes a large corpus of text documents for twitter, blogs, and news to discover the structure in the data to build a predictive text product.

## Application Usage  
* This basic Shiny product emulates a predictive text algorithm that evaluates the free text input of the user in real time.
  The application allows the user to specify the 'word history count' to be used in the prediction algorithm.

*  Input:
    * Text within text box and 'Number of Words to Evaluate' via the slider bar (Ranges from 1-5 with default of 5).    

* Output:
    * Plot of the Top 10 word sequences based upon the user input and probability of 'next word' witin the sequence.
    * A text box showing the words that are currently getting evaluated based on the user input.
    * A text box showing the 'predicted next word'.
   
## Data Analysis

* Source of data training sets: 
    * <a href="https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip">SwiftKey US Locale Data</a> 
    * <a href="https://www.cs.cmu.edu/~biglou/resources/bad-words.txt">Profanity Data Set</a>
    * The training data set is sampled at 20% and converted to sentences to better estimate word
      prediction that accounts for ngrams within sentence boundaries.
    * The sampled data set is then loaded into a corpus for efficient manipulation of text data.
    * Within the corpus, several procedures are used to clean and filter the data, including
      the conversion of non-ASCII text to ASCII text and conversion of all characters to lowercase.
      It also incluces the removal of profanity, punctuation, numbers, and whitespace.
   
## Prediction Algorithm / Methodology

* Once the cleaned corpus data is compiled, the RWeka package is used to tokenize the data into ngrams
  of length 1 thru 6. 
* The ngrams are compiled to serve as the 'dictionary' for this application in the form of .rds files.
* When the Shiny application is initialized, the 'dictionary' files are loaded and referenced as the 
  user enters the text and 'number of words to evaluate' input.
* The algorithm uses a backoff algorithm that starts with an attempt to match the longest word sequence
  from the user input.  If no match is found, then next longest sequence is evaluated.

## References

* <a href="https://jjoyce1000.shinyapps.io/NLP_Text_Prediction/">NLP Text Prediction Shiny Application</a> 
* <a href="https://github.com/jjoyce1000/Natural-Language-Processing">GitHub Repository</a> 
* <a href="http://rpubs.com/jjoyce1000/NLP_Presentation">NLP Text Prediction Presentation</a> 
* <a href="https://www.coursera.org/specializations/jhu-data-science">Johns Hopkins Data Science</a> 




